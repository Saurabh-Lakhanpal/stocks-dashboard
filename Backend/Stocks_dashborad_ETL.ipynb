{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 619029 entries, 0 to 619039\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   date         619029 non-null  object \n",
      " 1   open_price   619029 non-null  float64\n",
      " 2   high         619029 non-null  float64\n",
      " 3   low_price    619029 non-null  float64\n",
      " 4   close_price  619029 non-null  float64\n",
      " 5   volume       619029 non-null  int64  \n",
      " 6   ticker       619029 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 37.8+ MB\n",
      "==================================================================================\n",
      "Missing tickers in ticker_tb: ['AAL' 'AAP' 'ABC' 'ADS' 'AET' 'AGN' 'AIV' 'ALK' 'ALXN' 'AMG' 'ANDV'\n",
      " 'ANTM' 'APC' 'ARNC' 'ATVI' 'AYI' 'BBT' 'BHF' 'BHGE' 'BLL' 'CA' 'CBG'\n",
      " 'CBS' 'CELG' 'CERN' 'CHK' 'CMA' 'COG' 'COL' 'COTY' 'CSRA' 'CTL' 'CTXS'\n",
      " 'CXO' 'DISCA' 'DISCK' 'DISH' 'DPS' 'DRE' 'DWDP' 'DXC' 'ESRX' 'ETFC'\n",
      " 'EVHC' 'FBHS' 'FB' 'FISV' 'FLIR' 'FLR' 'FLS' 'FL' 'FTI' 'GGP' 'GPS' 'GT'\n",
      " 'HBI' 'HCN' 'HCP' 'HOG' 'HP' 'HRB' 'HRS' 'ILMN' 'INFO' 'JEC' 'JWN' 'KORS'\n",
      " 'KSS' 'KSU' 'LB' 'LEG' 'LLL' 'LNC' 'LUK' 'MAC' 'MAT' 'MON' 'MRO' 'MYL'\n",
      " 'M' 'NAVI' 'NBL' 'NFX' 'NLSN' 'NOV' 'NWL' 'PBCT' 'PCLN' 'PDCO' 'PKI'\n",
      " 'PRGO' 'PVH' 'PXD' 'PX' 'QRVO' 'RE' 'RHI' 'RHT' 'RRC' 'RTN' 'SCG' 'SEE'\n",
      " 'SIG' 'SLG' 'SNI' 'SRCL' 'STI' 'SYMC' 'TIF' 'TMK' 'TRIP' 'TSS' 'TWX'\n",
      " 'UAA' 'UA' 'UNM' 'UTX' 'VAR' 'VFC' 'VIAB' 'VNO' 'WHR' 'WLTW' 'WRK' 'WU'\n",
      " 'WYN' 'XEC' 'XLNX' 'XL' 'XRAY' 'XRX' 'ZION']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 503 entries, 0 to 502\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   ticker        503 non-null    object\n",
      " 1   company_name  503 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.0+ KB\n",
      "==================================================================================\n",
      "Database stocks_dashboard_db dropped successfully.\n",
      "Database stocks_dashboard_db created successfully.\n",
      "SQL schema file executed successfully.\n",
      "Data for ticker_tb uploaded successfully. Total rows: 503\n",
      "Data for sp500_tb uploaded successfully. Total rows: 619029\n",
      "Data for portfolio_tb uploaded successfully. Total rows: 6\n"
     ]
    }
   ],
   "source": [
    "#Stocks_dashboard_ETL.ipynb\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import requests\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import sql\n",
    "import shutil\n",
    "import stat\n",
    "\n",
    "# Set the Kaggle configuration directory to a custom path\n",
    "custom_kaggle_path = '.kaggle'\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = custom_kaggle_path\n",
    "\n",
    "# Define paths\n",
    "zip_file_path = 'Resources/sandp500.zip'\n",
    "extract_path = 'Resources/sandp500'\n",
    "\n",
    "# Function to handle permission errors\n",
    "def handle_remove_readonly(func, path, exc_info):\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)\n",
    "\n",
    "# Remove existing ZIP file if it exists\n",
    "if os.path.exists(zip_file_path):\n",
    "    os.remove(zip_file_path)\n",
    "\n",
    "# Remove existing folder and its contents if it exists\n",
    "if os.path.exists(extract_path):\n",
    "    shutil.rmtree(extract_path, onerror=handle_remove_readonly)\n",
    "\n",
    "# Download the S&P 500 dataset from Kaggle and store in the Resources folder\n",
    "os.system('kaggle datasets download -d camnugent/sandp500 -p Resources')\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "csv_file_path = os.path.join(extract_path, 'all_stocks_5yr.csv')\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Delete the ZIP file and extracted folder after loading the dataset\n",
    "os.remove(zip_file_path)\n",
    "shutil.rmtree(extract_path, onerror=handle_remove_readonly)\n",
    "\n",
    "# Remove rows with missing values\n",
    "cleaned_df = df.dropna()\n",
    "\n",
    "# Convert date column to datetime using .loc\n",
    "cleaned_df.loc[:, 'date'] = pd.to_datetime(cleaned_df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates (NaT values)\n",
    "cleaned_df = cleaned_df.dropna(subset=['date'])\n",
    "\n",
    "# Rename columns for clarity using .loc\n",
    "cleaned_df = cleaned_df.rename(columns={'Name': 'ticker', 'date': 'date', 'open': 'open_price', \n",
    "                                        'close': 'close_price', 'low': 'low_price', 'high_price': 'high_price', \n",
    "                                        'volume': 'volume'})\n",
    "\n",
    "# Convert date column back to datetime to ensure correct format for PostgreSQL\n",
    "cleaned_df.loc[:, 'date'] = pd.to_datetime(cleaned_df['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Display df info\n",
    "cleaned_df.info()\n",
    "print('==================================================================================')\n",
    "\n",
    "# Fetch S&P 500 tickers from Wikipedia\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "# Request page content with headers to avoid being blocked\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table', {'class': 'wikitable'}) \n",
    "    \n",
    "    if len(tables) > 0:\n",
    "        table = tables[0]\n",
    "        \n",
    "        # Wrap the table HTML string in a StringIO object\n",
    "        table_html = StringIO(str(table))\n",
    "        \n",
    "        # Read the table directly into a DataFrame using pandas\n",
    "        ticker_df = pd.read_html(table_html)[0]\n",
    "        \n",
    "        # Select only the Ticker Symbol and Company Name columns\n",
    "        ticker_df = ticker_df[['Symbol', 'Security']]\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        ticker_df.columns = ['ticker', 'company_name']\n",
    "        \n",
    "        # Check for duplicates and drop them\n",
    "        ticker_df = ticker_df.drop_duplicates()\n",
    "\n",
    "        # Check for missing/null values and drop rows with any missing values\n",
    "        ticker_df = ticker_df.dropna()\n",
    "\n",
    "        # Verify that all tickers in cleaned_df are present in ticker_df\n",
    "        missing_tickers = cleaned_df[~cleaned_df['ticker'].isin(ticker_df['ticker'])]['ticker'].unique()\n",
    "        if len(missing_tickers) > 0:\n",
    "            print(f\"Missing tickers in ticker_tb: {missing_tickers}\")\n",
    "\n",
    "        # Display DataFrame info after cleaning\n",
    "        ticker_df.info()\n",
    "    else:\n",
    "        print(\"Error: S&P 500 company table not found on Wikipedia.\")\n",
    "else:\n",
    "    print(f\"Error: Failed to fetch page, status code {response.status_code}\")\n",
    "print('==================================================================================')\n",
    "\n",
    "# Define initial and target database parameters\n",
    "initial_db_params = {\n",
    "    'dbname': 'postgres',  \n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "db_params = {\n",
    "    'dbname': 'stocks_dashboard_db',  \n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Functions for database operations\n",
    "def terminate_sessions(cursor, dbname):\n",
    "    cursor.execute(sql.SQL(\"\"\"\n",
    "        SELECT pg_terminate_backend(pid)\n",
    "        FROM pg_stat_activity\n",
    "        WHERE datname = %s AND pid <> pg_backend_pid();\n",
    "    \"\"\"), [dbname])\n",
    "\n",
    "def drop_database(cursor, dbname):\n",
    "    cursor.execute(\"SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = %s AND pid <> pg_backend_pid();\", [dbname])\n",
    "    cursor.execute(sql.SQL(\"DROP DATABASE IF EXISTS {}\").format(sql.Identifier(dbname)))\n",
    "\n",
    "def create_database(cursor, dbname):\n",
    "    cursor.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(dbname)))\n",
    "\n",
    "def execute_sql_file(cursor, sql_file_path):\n",
    "    with open(sql_file_path, 'r') as file:\n",
    "        sql_commands = file.read()\n",
    "    cursor.execute(sql.SQL(sql_commands))\n",
    "\n",
    "sql_file_path = 'stocks_dashboard_db_schema.sql'\n",
    "\n",
    "# Connect to the initial database and create the target database\n",
    "try:\n",
    "    # Connect to the initial database (postgres)\n",
    "    connection = psycopg2.connect(**initial_db_params)\n",
    "    connection.autocommit = True\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Drop the target database if it exists\n",
    "    drop_database(cursor, db_params['dbname'])\n",
    "    print(f\"Database {db_params['dbname']} dropped successfully.\")\n",
    "\n",
    "    # Create the target database\n",
    "    create_database(cursor, db_params['dbname'])\n",
    "    print(f\"Database {db_params['dbname']} created successfully.\")\n",
    "\n",
    "    # Close the initial connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # Connect to the newly created target database (stocks_dashboard_db)\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the SQL schema file to set up the database\n",
    "    execute_sql_file(cursor, sql_file_path)\n",
    "    connection.commit()\n",
    "    print(\"SQL schema file executed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if connection:\n",
    "        connection.rollback()\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "\n",
    "# Upload data from DataFrames to PostgreSQL\n",
    "def upload_df_to_table(connection_string, table_name, df):\n",
    "    engine = create_engine(connection_string)\n",
    "    df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    return len(df)\n",
    "\n",
    "# Ensure column names in cleaned_df and ticker_df match the table schema\n",
    "ticker_df.columns = ['ticker', 'company_name']\n",
    "cleaned_df.columns = ['date', 'open_price', 'close_price', 'low_price', 'high_price', 'volume', 'ticker']\n",
    "\n",
    "# Upload the DataFrames to PostgreSQL tables\n",
    "connection_string = f\"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "\n",
    "try:\n",
    "    ticker_rows = upload_df_to_table(connection_string, 'ticker_tb', ticker_df)\n",
    "    print(f\"Data for ticker_tb uploaded successfully. Total rows: {ticker_rows}\")\n",
    "    \n",
    "    sp500_rows = upload_df_to_table(connection_string, 'sp500_tb', cleaned_df)\n",
    "    print(f\"Data for sp500_tb uploaded successfully. Total rows: {sp500_rows}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Add fake data to portfolio_tb\n",
    "# Create a DataFrame with specific ticker symbols\n",
    "portfolio_data = {\n",
    "    'ticker': ['AAPL', 'NVDA', 'AMZN', 'TSLA', 'NFLX', 'MCD'],\n",
    "    'shares': [50, 30, 40, 25, 35, 45]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "portfolio_df = pd.DataFrame(portfolio_data)\n",
    "\n",
    "# Upload the DataFrame to PostgreSQL\n",
    "try:\n",
    "    engine = create_engine(connection_string)\n",
    "    portfolio_df.to_sql('portfolio_tb', engine, if_exists='append', index=False)\n",
    "    print(f\"Data for portfolio_tb uploaded successfully. Total rows: {len(portfolio_df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker  distinct_date_count             min_date             max_date\n",
      "0        A                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "1      AAL                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "2      AAP                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "3     AAPL                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "4     ABBV                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "..     ...                  ...                  ...                  ...\n",
      "500    XYL                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "501    YUM                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "502    ZBH                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "503   ZION                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "504    ZTS                 1259  2013-02-08 00:00:00  2018-02-07 00:00:00\n",
      "\n",
      "[505 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "result = cleaned_df.groupby('ticker').agg(\n",
    "    distinct_date_count=('date', 'nunique'),\n",
    "    min_date=('date', 'min'),\n",
    "    max_date=('date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
